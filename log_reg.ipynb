{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Union, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogReg():\n",
    "    \"\"\"\n",
    "    Линейная регрессия\n",
    "\n",
    "    Гиперпараметры:\n",
    "    ***************\n",
    "    n_iter : int, optional\n",
    "        Количество шагов градиентного спуска (default: 100)\n",
    "    learning_rate : Union[float, Callable], optional\n",
    "        Коэффициент скорости обучения градиентного спуска.\n",
    "        Если на вход пришла lambda-функция, то learning_rate вычисляется \n",
    "        на каждом шаге на основе переданной функцией (default: 0.01)\n",
    "    weights : np.ndarray, optional\n",
    "        Веса модели (default: None)\n",
    "    metric : str, optional\n",
    "        Метрика, которая будет вычисляться параллельно с функцией потерь.\n",
    "        Принимает одно из следующих значений: mae, mse, rmse, mape, r2 (default: None_\n",
    "    reg : str, optional\n",
    "        Вид регуляризации. Принимает одно из следующих значений: l1, l2, elasticnet (default: None)\n",
    "    l1_coef : float, optional\n",
    "        Коэффициент L1 регуляризации. Принимает значения от 0.0 до 1.0 (default: 0)\n",
    "    l2_coef : float, optional\n",
    "        Коэффициент L2 регуляризации. Принимает значения от 0.0 до 1.0 (default: 0)\n",
    "    sgd_sample : Union[int, float], optional\n",
    "        Количество образцов, которое будет использоваться на каждой итерации обучения.\n",
    "        Может принимать целые числа, либо дробные от 0.0 до 1.0 (default: None)\n",
    "    random_state : int, optional\n",
    "        Сид для воспроизводимости результата (default: 42)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                n_iter: int = 100,\n",
    "                learning_rate: float = 0.01,\n",
    "                weights: np.ndarray = None,\n",
    "                metric: str = None,\n",
    "                reg: str = None,\n",
    "                l1_coef: float = 0,\n",
    "                l2_coef: float = 0,\n",
    "                sgd_sample: float = None,\n",
    "                random_state: int = 42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.score = None\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        params = [f'{k}={v}' for k,v in self.__dict__.items()]\n",
    "        return 'MyLineReg class: ' + ', '.join(params)\n",
    "\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "        \"\"\"Средняя абсолютная ошибка\"\"\"\n",
    "        return (y_true - y_pred).abs().mean()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "        \"\"\"Среднеквадратичная ошибка\"\"\"\n",
    "        return ((y_true - y_pred)**2).mean()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def rmse(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "        \"\"\"Квадратный корень из среднеквадратичной ошибки.\"\"\"\n",
    "        return np.sqrt(((y_true - y_pred)**2).mean())\n",
    "class MyLogReg():\n",
    "    \"\"\"\n",
    "    Линейная регрессия\n",
    "\n",
    "    Гиперпараметры:\n",
    "    ***************\n",
    "    n_iter : int, optional\n",
    "        Количество шагов градиентного спуска (default: 100)\n",
    "    learning_rate : Union[float, Callable], optional\n",
    "        Коэффициент скорости обучения градиентного спуска.\n",
    "        Если на вход пришла lambda-функция, то learning_rate вычисляется \n",
    "        на каждом шаге на основе переданной функцией (default: 0.01)\n",
    "    weights : np.ndarray, optional\n",
    "        Веса модели (default: None)\n",
    "    metric : str, optional\n",
    "        Метрика, которая будет вычисляться параллельно с функцией потерь.\n",
    "        Принимает одно из следующих значений: mae, mse, rmse, mape, r2 (default: None_\n",
    "    reg : str, optional\n",
    "        Вид регуляризации. Принимает одно из следующих значений: l1, l2, elasticnet (default: None)\n",
    "    l1_coef : float, optional\n",
    "        Коэффициент L1 регуляризации. Принимает значения от 0.0 до 1.0 (default: 0)\n",
    "    l2_coef : float, optional\n",
    "        Коэффициент L2 регуляризации. Принимает значения от 0.0 до 1.0 (default: 0)\n",
    "    sgd_sample : Union[int, float], optional\n",
    "        Количество образцов, которое будет использоваться на каждой итерации обучения.\n",
    "        Может принимать целые числа, либо дробные от 0.0 до 1.0 (default: None)\n",
    "    random_state : int, optional\n",
    "        Сид для воспроизводимости результата (default: 42)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                n_iter: int = 100,\n",
    "                learning_rate: float = 0.01,\n",
    "                weights: np.ndarray = None,\n",
    "                metric: str = None,\n",
    "                reg: str = None,\n",
    "                l1_coef: float = 0,\n",
    "                l2_coef: float = 0,\n",
    "                sgd_sample: float = None,\n",
    "                random_state: int = 42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.score = None\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        params = [f'{k}={v}' for k,v in self.__dict__.items()]\n",
    "        return 'MyLogReg class: ' + ', '.join(params)\n",
    "\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid(self, value):\n",
    "\n",
    "        return 1 / (1 + np.exp(-value))\n",
    "    \n",
    "\n",
    "    def loss(self, y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Подсчет значения функции потерь (MSE) \n",
    "        \"\"\"\n",
    "        eps = 1e-15\n",
    "        loss = ((y_true * np.log(y_pred + eps)) + (1-y_true) * np.log(1 - y_pred + eps)).mean()\n",
    "        if self.reg == 'l1':\n",
    "            loss += self.l1_coef * np.abs(self.weights).sum()\n",
    "        elif self.reg == 'l2':\n",
    "            loss += self.l2_coef * np.square(self.weights).sum()\n",
    "        elif self.reg == 'elasticnet':\n",
    "            loss += self.l1_coef * np.abs(self.weights).sum() + self.l2_coef * np.square(self.weights).sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def gradient(self, X: pd.DataFrame, y_true: pd.Series, y_pred: np.ndarray, batch_idx: list) -> float:\n",
    "        \"\"\"\n",
    "        Подсчет градиента для очередного приближения с учетом регуляризации\n",
    "        \"\"\"\n",
    "        y_true = y_true.iloc[batch_idx]\n",
    "        y_pred = y_pred[batch_idx]\n",
    "        X = X.iloc[batch_idx]\n",
    "\n",
    "        gradient = 1 / len(y_true) * np.dot((y_pred - y_true), X)\n",
    "\n",
    "        if self.reg == 'l1':\n",
    "            gradient += self.l1_coef * np.sign(self.weights)\n",
    "        elif self.reg == 'l2':\n",
    "            gradient += self.l2_coef * 2 * self.weights\n",
    "        elif self.reg == 'elasticnet':\n",
    "            gradient += self.l1_coef * np.sign(self.weights) + self.l2_coef * 2 * self.weights \n",
    "\n",
    "        return gradient\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose: int = False):\n",
    "        \"\"\"\n",
    "        Обучение линейной регрессии\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Все фичи\n",
    "        y : pd.Series\n",
    "            Целевая переменная\n",
    "        verbose : int, optional\n",
    "            Указывает через сколько итераций градиентного спуска будет выводиться лог\n",
    "        \"\"\"\n",
    "        #random.seed(self.random_state)\n",
    "        features = X.copy()\n",
    "        features.insert(loc=0, column='x0', value=1)\n",
    "        self.weights = np.ones(features.shape[1])\n",
    "\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            #if isinstance(self.sgd_sample, int):\n",
    "                #batch_idx = random.sample(range(features.shape[0]), self.sgd_sample)\n",
    "            #elif isinstance(self.sgd_sample, float):\n",
    "                #batch_idx = random.sample(range(features.shape[0]), int(features.shape[0] * self.sgd_sample))\n",
    "            #else:\n",
    "            batch_idx = list(range(features.shape[0]))\n",
    "            \n",
    "            y_pred = self.sigmoid(np.dot(features, self.weights))\n",
    "            loss = self.loss(y, y_pred)\n",
    "            gradient = self.gradient(features, y, y_pred, batch_idx)\n",
    "\n",
    "            if callable(self.learning_rate):\n",
    "                self.weights -= self.learning_rate(i) * gradient\n",
    "            else:\n",
    "                self.weights -= self.learning_rate * gradient\n",
    "            #self.score = MyLogReg.get_metric_score(y, self.predict(X), self.metric)\n",
    "\n",
    "            if verbose:\n",
    "                if (i == 1) or (i % verbose == 0):\n",
    "                    log = f'{i} | loss: {loss}'\n",
    "                    #if self.metric:\n",
    "                        #log += f' | metric: {MyLogReg.get_metric_score(y, y_pred, self.metric)}'\n",
    "                    print(log)\n",
    "\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Выдача предсказаний моделью\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Матрица фичей\n",
    "        \"\"\"\n",
    "        features = X.copy()\n",
    "        features.insert(loc=0, column='x0', value=1)\n",
    "        y_pred = np.dot(features, self.weights)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def get_coef(self):\n",
    "        return self.weights[1:]\n",
    "\n",
    "\n",
    "    def get_best_score(self):\n",
    "        return self.score\n",
    "        \"\"\"Коэффициент детерминации\"\"\"\n",
    "        return 1 - ((y_true - y_pred)**2).mean() / ((y_true - y_true.mean())**2).mean()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def mape(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "        \"\"\"Средняя абсолютная ошибка в процентах \"\"\"\n",
    "        return 100 * ((y_true - y_pred) / y_true).abs().mean()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_metric_score(y_true: pd.Series, y_pred: pd.Series, metric: str):\n",
    "        \"\"\" \n",
    "        Подсчет выбранной метрики\n",
    "        \"\"\"\n",
    "        if metric == 'mae':\n",
    "            return MyLineReg.mae(y_true, y_pred)\n",
    "        elif metric == 'mse':\n",
    "            return MyLineReg.mse(y_true, y_pred)\n",
    "        elif metric == 'rmse':\n",
    "            return MyLineReg.rmse(y_true, y_pred)\n",
    "        elif metric == 'r2':\n",
    "            return MyLineReg.r2(y_true, y_pred)\n",
    "        elif metric == 'mape':\n",
    "            return MyLineReg.mape(y_true, y_pred)\n",
    "        else:\n",
    "            ValueError(\"Wrong metric name\")\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid(self, value):\n",
    "\n",
    "        return 1 / (1 + np.exp(-value))\n",
    "    \n",
    "\n",
    "    def loss(self, y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Подсчет значения функции потерь (MSE) \n",
    "        \"\"\"\n",
    "        eps = 1e-15\n",
    "        loss = ((y_true * np.log(y_pred + eps)) + (1-y_true) * np.log(1 - y_pred + eps)).mean()\n",
    "        if self.reg == 'l1':\n",
    "            loss += self.l1_coef * np.abs(self.weights).sum()\n",
    "        elif self.reg == 'l2':\n",
    "            loss += self.l2_coef * np.square(self.weights).sum()\n",
    "        elif self.reg == 'elasticnet':\n",
    "            loss += self.l1_coef * np.abs(self.weights).sum() + self.l2_coef * np.square(self.weights).sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def gradient(self, X: pd.DataFrame, y_true: pd.Series, y_pred: np.ndarray, batch_idx: list) -> float:\n",
    "        \"\"\"\n",
    "        Подсчет градиента для очередного приближения с учетом регуляризации\n",
    "        \"\"\"\n",
    "        y_true = y_true.iloc[batch_idx]\n",
    "        y_pred = y_pred[batch_idx]\n",
    "        X = X.iloc[batch_idx]\n",
    "\n",
    "        gradient = 1 / len(y_true) * np.dot((y_pred - y_true), X)\n",
    "\n",
    "        if self.reg == 'l1':\n",
    "            gradient += self.l1_coef * np.sign(self.weights)\n",
    "        elif self.reg == 'l2':\n",
    "            gradient += self.l2_coef * 2 * self.weights\n",
    "        elif self.reg == 'elasticnet':\n",
    "            gradient += self.l1_coef * np.sign(self.weights) + self.l2_coef * 2 * self.weights \n",
    "\n",
    "        return gradient\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose: int = False):\n",
    "        \"\"\"\n",
    "        Обучение линейной регрессии\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Все фичи\n",
    "        y : pd.Series\n",
    "            Целевая переменная\n",
    "        verbose : int, optional\n",
    "            Указывает через сколько итераций градиентного спуска будет выводиться лог\n",
    "        \"\"\"\n",
    "        random.seed(self.random_state)\n",
    "        features = X.copy()\n",
    "        features.insert(loc=0, column='x0', value=1)\n",
    "        self.weights = np.ones(features.shape[1])\n",
    "\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            if isinstance(self.sgd_sample, int):\n",
    "                batch_idx = random.sample(range(features.shape[0]), self.sgd_sample)\n",
    "            elif isinstance(self.sgd_sample, float):\n",
    "                batch_idx = random.sample(range(features.shape[0]), int(features.shape[0] * self.sgd_sample))\n",
    "            else:\n",
    "                batch_idx = list(range(features.shape[0]))\n",
    "            \n",
    "            y_pred = self.sigmoid(np.dot(features, self.weights))\n",
    "            loss = self.loss(y, y_pred)\n",
    "            gradient = self.gradient(features, y, y_pred, batch_idx)\n",
    "\n",
    "            if callable(self.learning_rate):\n",
    "                self.weights -= self.learning_rate(i) * gradient\n",
    "            else:\n",
    "                self.weights -= self.learning_rate * gradient\n",
    "            self.score = MyLineReg.get_metric_score(y, self.predict(X), self.metric)\n",
    "\n",
    "            if verbose:\n",
    "                if (i == 1) or (i % verbose == 0):\n",
    "                    log = f'{i} | loss: {loss}'\n",
    "                    if self.metric:\n",
    "                        log += f' | metric: {MyLineReg.get_metric_score(y, y_pred, self.metric)}'\n",
    "                    print(log)\n",
    "\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Выдача предсказаний моделью\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Матрица фичей\n",
    "        \"\"\"\n",
    "        features = X.copy()\n",
    "        features.insert(loc=0, column='x0', value=1)\n",
    "        y_pred = np.dot(features, self.weights)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def get_coef(self):\n",
    "        return self.weights[1:]\n",
    "\n",
    "\n",
    "    def get_best_score(self):\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogReg():\n",
    "    \"\"\"\n",
    "    Линейная регрессия\n",
    "\n",
    "    Гиперпараметры:\n",
    "    ***************\n",
    "    n_iter : int, optional\n",
    "        Количество шагов градиентного спуска (default: 100)\n",
    "    learning_rate : Union[float, Callable], optional\n",
    "        Коэффициент скорости обучения градиентного спуска.\n",
    "        Если на вход пришла lambda-функция, то learning_rate вычисляется \n",
    "        на каждом шаге на основе переданной функцией (default: 0.01)\n",
    "    weights : np.ndarray, optional\n",
    "        Веса модели (default: None)\n",
    "    metric : str, optional\n",
    "        Метрика, которая будет вычисляться параллельно с функцией потерь.\n",
    "        Принимает одно из следующих значений: mae, mse, rmse, mape, r2 (default: None_\n",
    "    reg : str, optional\n",
    "        Вид регуляризации. Принимает одно из следующих значений: l1, l2, elasticnet (default: None)\n",
    "    l1_coef : float, optional\n",
    "        Коэффициент L1 регуляризации. Принимает значения от 0.0 до 1.0 (default: 0)\n",
    "    l2_coef : float, optional\n",
    "        Коэффициент L2 регуляризации. Принимает значения от 0.0 до 1.0 (default: 0)\n",
    "    sgd_sample : Union[int, float], optional\n",
    "        Количество образцов, которое будет использоваться на каждой итерации обучения.\n",
    "        Может принимать целые числа, либо дробные от 0.0 до 1.0 (default: None)\n",
    "    random_state : int, optional\n",
    "        Сид для воспроизводимости результата (default: 42)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                n_iter: int = 100,\n",
    "                learning_rate: float = 0.01,\n",
    "                weights: np.ndarray = None,\n",
    "                metric: str = None,\n",
    "                reg: str = None,\n",
    "                l1_coef: float = 0,\n",
    "                l2_coef: float = 0,\n",
    "                sgd_sample: float = None,\n",
    "                random_state: int = 42):\n",
    "        #self.verbose = verbose\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.score = None\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        params = [f'{k}={v}' for k,v in self.__dict__.items()]\n",
    "        return 'MyLogReg class: ' + ', '.join(params)\n",
    "\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(tp, tn, fp, fn) -> float:\n",
    "        \"\"\"\"\"\"\n",
    "        return (tp + tn) / (tp + tn + fp + fn) \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def precision(tp, fp) -> float:\n",
    "        \"\"\"\"\"\"\n",
    "        return tp / (tp + fp) \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def recall(tp, fn) -> float:\n",
    "        \"\"\"\"\"\"\n",
    "        return tp / (tp + fn)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def f1(precision, recall) -> float:\n",
    "        \"\"\"\"\"\"\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def roc_auc(y_true: pd.Series, y_pred: pd.Series, y_pred_proba: pd.Series) -> float:\n",
    "        \"\"\"\"\"\"\n",
    "        y_pred_proba = pd.DataFrame(y_pred_proba.round(10))\n",
    "        df = pd.concat([y_pred_proba, y_true], axis=1, names=[0,1])\n",
    "        print(df)\n",
    "        df = df.sort_values(by=0, ascending=False)\n",
    "\n",
    "        positives = df[df[1] == 1]\n",
    "        negatives = df[df[1] == 0]\n",
    "\n",
    "        total = 0\n",
    "        for current_score in negatives[0]:\n",
    "            score_higher = (positives[0] > current_score).sum()\n",
    "            score_equal = (positives[0] == current_score).sum()\n",
    "            total += score_higher + 0.5 * score_equal\n",
    "\n",
    "        return total / (positives.shape[0] * negatives.shape[0])\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_metric_score(y_true: pd.Series, y_pred: pd.Series, y_pred_proba: pd.Series, metric: str):\n",
    "        \"\"\" \n",
    "        Подсчет выбранной метрики\n",
    "        \"\"\"\n",
    "\n",
    "        negative = 0\n",
    "        positive = 1\n",
    "\n",
    "        tp = np.sum(np.logical_and(y_pred == positive, y_true == positive))\n",
    "        tn = np.sum(np.logical_and(y_pred == negative, y_true == negative))\n",
    "        fp = np.sum(np.logical_and(y_pred == positive, y_true == negative))\n",
    "        fn = np.sum(np.logical_and(y_pred == negative, y_true == positive))\n",
    "\n",
    "        if metric == 'accuracy':\n",
    "            return MyLogReg.accuracy(tp, tn, fp, fn)\n",
    "        elif metric == 'precision':\n",
    "            return MyLogReg.precision(tp, fp)\n",
    "        elif metric == 'recall':\n",
    "            return MyLogReg.recall(tp, fn)\n",
    "        elif metric == 'f1':\n",
    "            return MyLogReg.f1(MyLogReg.precision(tp, fp), MyLogReg.recall(tp, fn))\n",
    "        elif metric == 'roc_auc':\n",
    "            return MyLogReg.roc_auc(y_true, y_pred, y_pred_proba)\n",
    "        else:\n",
    "            ValueError(\"Wrong metric name\")\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid(self, value):\n",
    "\n",
    "        return 1 / (1 + np.exp(-value))\n",
    "    \n",
    "\n",
    "    def loss(self, y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Подсчет значения функции потерь (MSE) \n",
    "        \"\"\"\n",
    "        eps = 1e-15\n",
    "        loss = ((y_true * np.log(y_pred + eps)) + (1-y_true) * np.log(1 - y_pred + eps)).mean()\n",
    "        if self.reg == 'l1':\n",
    "            loss += self.l1_coef * np.abs(self.weights).sum()\n",
    "        elif self.reg == 'l2':\n",
    "            loss += self.l2_coef * np.square(self.weights).sum()\n",
    "        elif self.reg == 'elasticnet':\n",
    "            loss += self.l1_coef * np.abs(self.weights).sum() + self.l2_coef * np.square(self.weights).sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def gradient(self, X: pd.DataFrame, y_true: pd.Series, y_pred: np.ndarray, batch_idx: list) -> float:\n",
    "        \"\"\"\n",
    "        Подсчет градиента для очередного приближения с учетом регуляризации\n",
    "        \"\"\"\n",
    "        y_true = y_true.iloc[batch_idx]\n",
    "        y_pred = y_pred[batch_idx]\n",
    "        X = X.iloc[batch_idx]\n",
    "\n",
    "        gradient = 1 / len(y_true) * np.dot((y_pred - y_true), X)\n",
    "\n",
    "        if self.reg == 'l1':\n",
    "            gradient += self.l1_coef * np.sign(self.weights)\n",
    "        elif self.reg == 'l2':\n",
    "            gradient += self.l2_coef * 2 * self.weights\n",
    "        elif self.reg == 'elasticnet':\n",
    "            gradient += self.l1_coef * np.sign(self.weights) + self.l2_coef * 2 * self.weights \n",
    "\n",
    "        return gradient\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose: int = True):\n",
    "        \"\"\"\n",
    "        Обучение линейной регрессии\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Все фичи\n",
    "        y : pd.Series\n",
    "            Целевая переменная\n",
    "        verbose : int, optional\n",
    "            Указывает через сколько итераций градиентного спуска будет выводиться лог\n",
    "        \"\"\"\n",
    "        #random.seed(self.random_state)\n",
    "        features = X.copy()\n",
    "        features.insert(loc=0, column='x0', value=1)\n",
    "        self.weights = np.ones(features.shape[1])\n",
    "\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            #if isinstance(self.sgd_sample, int):\n",
    "                #batch_idx = random.sample(range(features.shape[0]), self.sgd_sample)\n",
    "            #elif isinstance(self.sgd_sample, float):\n",
    "                #batch_idx = random.sample(range(features.shape[0]), int(features.shape[0] * self.sgd_sample))\n",
    "            #else:\n",
    "            batch_idx = list(range(features.shape[0]))\n",
    "            \n",
    "            y_pred_proba = self.sigmoid(np.dot(features, self.weights))\n",
    "            y_pred = (self.predict_proba(X) > 0.5).astype(int)\n",
    "            loss = self.loss(y, y_pred_proba)\n",
    "            gradient = self.gradient(features, y, y_pred_proba, batch_idx)\n",
    "\n",
    "            if callable(self.learning_rate):\n",
    "                self.weights -= self.learning_rate(i) * gradient\n",
    "            else:\n",
    "                self.weights -= self.learning_rate * gradient\n",
    "            #self.score = MyLogReg.get_metric_score(y, self.predict(X), self.predict_proba(X), self.metric)\n",
    "\n",
    "            if verbose:\n",
    "                if (i == 1) or (i % verbose == 0):\n",
    "                    log = f'{i} | loss: {loss}'\n",
    "                    if self.metric:\n",
    "                        log += f' | metric: {MyLogReg.get_metric_score(y, y_pred, y_pred_proba, self.metric)}'\n",
    "                    print(log)\n",
    "        self.score = MyLogReg.get_metric_score(y, y_pred, y_pred_proba, self.metric)\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Выдача предсказаний моделью\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Матрица фичей\n",
    "        \"\"\"\n",
    "        features = X.copy()\n",
    "        features.insert(loc=0, column='x0', value=1)\n",
    "        y_pred = self.sigmoid(np.dot(features, self.weights))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Выдача предсказаний моделью\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Матрица фичей\n",
    "        \"\"\"\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "    \n",
    "\n",
    "    def get_coef(self):\n",
    "        return self.weights[1:]\n",
    "\n",
    "\n",
    "    def get_best_score(self):\n",
    "        return self.score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41 43]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([41, 42, 43, 44])\n",
    "\n",
    "x = [True, False, True, False]\n",
    "\n",
    "newarr = arr[x]\n",
    "\n",
    "print(newarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=14, n_informative=10, noise=15, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "X.columns = [f'col_{col}' for col in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0           0\n",
      "0    0.572953  -48.005272\n",
      "1    0.711586  145.801614\n",
      "2    0.951644  -49.114775\n",
      "3    0.995568   24.902238\n",
      "4    0.232790 -152.611643\n",
      "..        ...         ...\n",
      "995  0.905670   57.484793\n",
      "996  0.494608   19.204280\n",
      "997  0.990122   95.095216\n",
      "998  0.039900 -107.973750\n",
      "999  0.999148  200.616716\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The column label '0' is not unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14712/2646947362.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLogReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprdictions_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14712/3567940768.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, verbose)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                     \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{i} | loss: {loss}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         \u001b[0mlog\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf' | metric: {MyLogReg.get_metric_score(y, y_pred, y_pred_proba, self.metric)}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLogReg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14712/3567940768.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(y_true, y_pred, y_pred_proba, metric)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMyLogReg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMyLogReg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyLogReg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyLogReg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mMyLogReg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong metric name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14712/3567940768.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(y_true, y_pred, y_pred_proba)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0my_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_proba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mpositives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mnegatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6940\u001b[0m             )\n\u001b[1;32m   6941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6942\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6944\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6946\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6947\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m                 \u001b[0mmulti_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m             \u001b[0mlabel_axis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"column\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1859\u001b[0m                 \u001b[0;34mf\"The {label_axis_name} label '{key}' is not unique.{multi_message}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m             )\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The column label '0' is not unique."
     ]
    }
   ],
   "source": [
    "model = MyLogReg(metric='roc_auc')\n",
    "model.fit(X,y,verbose=5)\n",
    "predictions = model.predict(X)\n",
    "prdictions_proba = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = np.array([0.86, 0.51, 0.78, 0.6, 0.6, 0.91,0.55, 0.46, 0.42])\n",
    "y_pred = np.array([1,0,0,1,0,1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba.round(10).argsort()\n",
    "y_pred[idx_sort[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.91],\n",
       "       [0.  , 0.86],\n",
       "       [0.  , 0.78],\n",
       "       [0.  , 0.6 ],\n",
       "       [1.  , 0.6 ],\n",
       "       [1.  , 0.55],\n",
       "       [0.  , 0.51],\n",
       "       [0.  , 0.46],\n",
       "       [0.  , 0.42]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array = np.stack((y_pred, y_pred_proba), axis=1)\n",
    "\n",
    "output_array=output_array[output_array[:, 1].argsort()][::-1]\n",
    "\n",
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([1,0,0,1,0,1,0,0,0])\n",
    "y_pred_proba = np.array([0.91, 0.86, 0.78, 0.6, 0.6, 0.55,0.51,0.46,0.42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.333333333333332 14.0\n"
     ]
    }
   ],
   "source": [
    "P = sum(y_pred)\n",
    "N = len(y_pred) - sum(y_pred)\n",
    "positive_count = 0\n",
    "indicators = 0\n",
    "# list of positive values\n",
    "#print(y_pred_proba)\n",
    "for score in -np.sort(-y_pred_proba.round(10)):\n",
    "    if score <= 0.5:\n",
    "        positive_count = len([x for x in -np.sort(-positive_list.round(10)) if x > score])\n",
    "        # number of positive classes with same score\n",
    "        counts = len(positive_list[positive_list==score]) / 2\n",
    "        indicators += positive_count + counts\n",
    "roc_auc = (1 / P * N) * indicators\n",
    "#return (1 / P * N) * indicators\n",
    "print (roc_auc, indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "1.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted_list = -np.sort(-y_pred_proba.round(10))\n",
    "\n",
    "sorted_list= np.stack((y_pred, y_pred_proba.round(10)), axis=1)\n",
    "\n",
    "sorted_list=sorted_list[sorted_list[:, 1].argsort()][::-1]\n",
    "\n",
    "positive_list = sorted_list[y_pred == 1]\n",
    "for score, clas in zip(sorted_list, y_pred,):\n",
    "    if clas == 0:\n",
    "        print(len(positive_list[positive_list==score]) / 2)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([1,0,0,1,0,1,0,0,0])\n",
    "y_pred_proba = np.array([0.91, 0.86, 0.78, 0.6, 0.6, 0.55,0.51,0.46,0.42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([1,0,0,1,0,1,0,0,0])\n",
    "y_pred_proba = np.array([0.77, 0.86, 0.78, 0.6, 0.6, 0.55,0.91,0.46,0.42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.5 0.5\n",
      "4.5 0.0\n",
      "7.5 0.0\n"
     ]
    }
   ],
   "source": [
    "P = sum(y_pred)\n",
    "N = len(y_pred) - sum(y_pred)\n",
    "positive_count = 0\n",
    "indicators = 0\n",
    "sorted_list= np.stack((y_pred, y_pred_proba.round(10)), axis=1)\n",
    "sorted_list=sorted_list[sorted_list[:, 1].argsort()][::-1]\n",
    "for elem in sorted_list:\n",
    "    if elem[0] == 0:\n",
    "        # number of positive classes before\n",
    "        positive_count = len([pos for pos in sorted_list if pos[0] == 1 and pos[1]>elem[1]])\n",
    "        # number of positive classes with same score\n",
    "        counts = len([pos for pos in sorted_list if pos[0] == 1 and pos[1]==elem[1]]) / 2\n",
    "        indicators += positive_count + counts\n",
    "        print(indicators, counts)\n",
    "roc_auc =  indicators / (P * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.  , 0.77]), array([1. , 0.6]), array([1.  , 0.55])]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pos for pos in sorted_list if pos[0] == 1 and pos[1]>elem[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166666666666667"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
